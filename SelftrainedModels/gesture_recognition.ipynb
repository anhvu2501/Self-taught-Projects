{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import random\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import cv2\n",
    "import mediapipe as mp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make numpy values easier to read\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "root_path = '/content/drive/My Drive/Colab Notebooks/course_dataset/'\n",
    "gestures= ['A','B','C','L','U','R']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = '/content/drive/My Drive/Colab Notebooks/course_dataset/ASL_letter_A/annotations.csv'\n",
    "ASL_train = pd.read_csv(path,\n",
    "      names=[\"ID\",\"frame\", \"gesture\", \"joint\", \"person_idx\", \"video_idx\", \"x\", \"y\"])\n",
    "\n",
    "  # Drop unneeded columns\n",
    "group_train = ASL_train.drop(['ID', 'gesture', 'joint', 'video_idx'], axis=1)\n",
    "\n",
    "  # Group and re-index by person_idx and frame\n",
    "grouped_out = group_train.set_index(['person_idx', 'frame', group_train.groupby(['person_idx', 'frame']).cumcount()+1]).unstack().sort_index(level=1, axis=1)\n",
    "\n",
    "  # Remove and reset the new indexes (Which were person_idx + frame)\n",
    "grouped_out.reset_index(drop=True, inplace=True)\n",
    "\n",
    "grouped_out.columns = grouped_out.columns.to_flat_index()\n",
    "\n",
    "grouped_out.drop(grouped_out.tail(1).index,inplace=True) # drop last n rows\n",
    "\n",
    "# Add the gesture\n",
    "grouped_out['gesture']=0\n",
    "\n",
    "gestures= ['B','C','L','U','R']\n",
    "finalDF = grouped_out\n",
    "\n",
    "for gestureIndex, g in enumerate(gestures):\n",
    "  index = gestureIndex + 1\n",
    "  path = '/content/drive/My Drive/Colab Notebooks/course_dataset/ASL_letter_{}/annotations.csv'.format(g)\n",
    "  ASL_train = pd.read_csv(\n",
    "     path,\n",
    "      names=[\"ID\",\"frame\", \"gesture\", \"joint\", \"person_idx\", \"video_idx\", \"x\", \"y\"])\n",
    "\n",
    "  # Drop unneeded columns\n",
    "  group_train = ASL_train.drop(['ID', 'gesture', 'joint', 'video_idx'], axis=1)\n",
    "\n",
    "  # Group and re-index by person_idx and frame\n",
    "  grouped_out = group_train.set_index(['person_idx', 'frame', group_train.groupby(['person_idx', 'frame']).cumcount()+1]).unstack().sort_index(level=1, axis=1)\n",
    "\n",
    "  # Remove and reset the new indexes (Which were person_idx + frame)\n",
    "  grouped_out.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  grouped_out.columns = grouped_out.columns.to_flat_index()\n",
    "\n",
    "  grouped_out.drop(grouped_out.tail(1).index,inplace=True) # drop last n rows\n",
    "  #print(len(grouped_out))\n",
    "  # Add the gesture\n",
    "  print(index)\n",
    "  grouped_out['gesture']=index\n",
    "  grouped_out\n",
    "  finalDF = finalDF.append(grouped_out)\n",
    "finalDF = finalDF.reset_index(drop=True)\n",
    "finalDF = finalDF.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(finalDF.shape)\n",
    "print(finalDF)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import keras.layers as layers\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare data\n",
    "ASL_features = finalDF.copy()\n",
    "ASL_labels = np.array(ASL_features.pop('gesture'))\n",
    "\n",
    "ASL_features_np = np.array(ASL_features).astype('float32')\n",
    "ASL_features_np = ASL_features_np.tolist()\n",
    "ASL_features_np = np.array(ASL_features_np)\n",
    "\n",
    "# Split into test, train & validaiton\n",
    "X = ASL_features_np\n",
    "y = ASL_labels\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(1024, activation = 'relu',use_bias=True))\n",
    "model.add(layers.Dropout(0.6))\n",
    "model.add(layers.Dense(256, activation = 'relu'))\n",
    "model.add(layers.Dense(6, activation = 'softmax'))\n",
    "\n",
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "hist = model.fit(x=x_train, y=y_train, validation_data=(x_val, y_val), batch_size=256,  epochs=15)\n",
    "\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "gesture_recognition.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}